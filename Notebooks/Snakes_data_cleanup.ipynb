{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cb0974-34b0-4a23-8c5e-70c2569f5187",
   "metadata": {},
   "source": [
    "Snakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c8c636a-9e1c-4780-adf7-8184822a3862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20226, 11)\n",
      "Index(['Request Number', 'Date Opened', 'Request ', 'Description',\n",
      "       'Incident Address', 'Zip Code', 'Trash Hauler', 'Trash Route',\n",
      "       'Council District', 'State Plan X', 'State Plan Y'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Request Number</th>\n",
       "      <th>Date Opened</th>\n",
       "      <th>Request</th>\n",
       "      <th>Description</th>\n",
       "      <th>Incident Address</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Trash Hauler</th>\n",
       "      <th>Trash Route</th>\n",
       "      <th>Council District</th>\n",
       "      <th>State Plan X</th>\n",
       "      <th>State Plan Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25270</td>\n",
       "      <td>11/01/17</td>\n",
       "      <td>Trash - Backdoor</td>\n",
       "      <td>house with the wheel chair ramp, they share dr...</td>\n",
       "      <td>3817 Crouch Dr</td>\n",
       "      <td>37207.0</td>\n",
       "      <td>RED RIVER</td>\n",
       "      <td>3205</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.727970e+06</td>\n",
       "      <td>686779.478089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25274</td>\n",
       "      <td>11/01/17</td>\n",
       "      <td>Trash - Curbside/Alley Missed Pickup</td>\n",
       "      <td>Curb/Trash miss Tuesday.</td>\n",
       "      <td>4028 Clarksville Pike</td>\n",
       "      <td>37218.0</td>\n",
       "      <td>RED RIVER</td>\n",
       "      <td>4202</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.721259e+06</td>\n",
       "      <td>685444.799565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25276</td>\n",
       "      <td>11/01/17</td>\n",
       "      <td>Trash - Curbside/Alley Missed Pickup</td>\n",
       "      <td>Curb/trash miss Tuesday.</td>\n",
       "      <td>6528 Thunderbird Dr</td>\n",
       "      <td>37209.0</td>\n",
       "      <td>RED RIVER</td>\n",
       "      <td>4205</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.707027e+06</td>\n",
       "      <td>659887.471571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25307</td>\n",
       "      <td>11/01/17</td>\n",
       "      <td>Trash - Curbside/Alley Missed Pickup</td>\n",
       "      <td>missed</td>\n",
       "      <td>2603 old matthews rd</td>\n",
       "      <td>37207.0</td>\n",
       "      <td>WASTE IND</td>\n",
       "      <td>2206</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.735692e+06</td>\n",
       "      <td>685027.245923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25312</td>\n",
       "      <td>11/01/17</td>\n",
       "      <td>Trash - Curbside/Alley Missed Pickup</td>\n",
       "      <td>Missed the even side of the road.</td>\n",
       "      <td>604 croley dr</td>\n",
       "      <td>37209.0</td>\n",
       "      <td>RED RIVER</td>\n",
       "      <td>4203</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.710186e+06</td>\n",
       "      <td>664205.101066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Request Number Date Opened                              Request   \\\n",
       "0           25270    11/01/17                      Trash - Backdoor   \n",
       "1           25274    11/01/17  Trash - Curbside/Alley Missed Pickup   \n",
       "2           25276    11/01/17  Trash - Curbside/Alley Missed Pickup   \n",
       "3           25307    11/01/17  Trash - Curbside/Alley Missed Pickup   \n",
       "4           25312    11/01/17  Trash - Curbside/Alley Missed Pickup   \n",
       "\n",
       "                                         Description       Incident Address  \\\n",
       "0  house with the wheel chair ramp, they share dr...         3817 Crouch Dr   \n",
       "1                           Curb/Trash miss Tuesday.  4028 Clarksville Pike   \n",
       "2                           Curb/trash miss Tuesday.    6528 Thunderbird Dr   \n",
       "3                                             missed   2603 old matthews rd   \n",
       "4                  Missed the even side of the road.          604 croley dr   \n",
       "\n",
       "   Zip Code Trash Hauler Trash Route  Council District  State Plan X  \\\n",
       "0   37207.0    RED RIVER        3205               2.0  1.727970e+06   \n",
       "1   37218.0    RED RIVER        4202               1.0  1.721259e+06   \n",
       "2   37209.0    RED RIVER        4205              20.0  1.707027e+06   \n",
       "3   37207.0    WASTE IND        2206               2.0  1.735692e+06   \n",
       "4   37209.0    RED RIVER        4203              20.0  1.710186e+06   \n",
       "\n",
       "    State Plan Y  \n",
       "0  686779.478089  \n",
       "1  685444.799565  \n",
       "2  659887.471571  \n",
       "3  685027.245923  \n",
       "4  664205.101066  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "trash_df = pd.read_csv('../data/trash_hauler_report.csv')\n",
    "#Explore the data\n",
    "print(trash_df.shape)\n",
    "print(trash_df.columns)\n",
    "trash_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b005d40-55ab-4567-9067-9d7a63c851f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I noticed extra spaces in some of the raw data, so this will clean up leading/trailing and extra spaces.\n",
    "\n",
    "#Clean only columns with object (string) type\n",
    "str_cols = trash_df.select_dtypes(include='object').columns\n",
    "\n",
    "#Clean and normalize whitespace in each string column\n",
    "for col in str_cols:\n",
    "    trash_df[col] = trash_df[col].astype(str).apply(lambda x: ' '.join(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e39f62-349f-47b1-a0f3-aac657b428ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all lowercase so matches are consistent\n",
    "trash_df['description_clean'] = trash_df['Description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29e17e34-c049-451a-997a-2d0c623de155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To help ensure data is complete, this creates a new Street Name column with only the street names (no address numbers)\n",
    "#Then if data is missing (Zip, Hauler, Route, District etc) it will complete the missing data based on the assumption that all addresses on the same street\n",
    "#uses the same Zip, Hauler, Route, District etc). It will complete any missing data for Incidents on the same Street Name.\n",
    "#It normalizes the new Street Name to USPS naming conventions so they can be easily grouped if needed.\n",
    "\n",
    "# USPS street suffix abbreviations\n",
    "suffix_map = {\n",
    "    'avenue': 'Ave', 'av': 'Ave', 'av.': 'Ave',\n",
    "    'road': 'Rd', 'rd': 'Rd',\n",
    "    'street': 'St', 'st': 'St',\n",
    "    'boulevard': 'Blvd', 'blvd': 'Blvd',\n",
    "    'drive': 'Dr', 'dr': 'Dr',\n",
    "    'court': 'Ct', 'ct': 'Ct',\n",
    "    'lane': 'Ln', 'ln': 'Ln',\n",
    "    'place': 'Pl', 'pl': 'Pl',\n",
    "    'circle': 'Cir', 'cir': 'Cir',\n",
    "    'trail': 'Trl', 'trl': 'Trl',\n",
    "    'parkway': 'Pkwy', 'pkwy': 'Pkwy',\n",
    "    'terrace': 'Ter', 'ter': 'Ter',\n",
    "    'way': 'Way',\n",
    "    'loop': 'Loop'\n",
    "}\n",
    "\n",
    "# USPS directional abbreviations\n",
    "direction_map = {\n",
    "    'north': 'N', 'n.': 'N', 'n': 'N',\n",
    "    'south': 'S', 's.': 'S', 's': 'S',\n",
    "    'east': 'E', 'e.': 'E', 'e': 'E',\n",
    "    'west': 'W', 'w.': 'W', 'w': 'W'\n",
    "}\n",
    "\n",
    "def normalize_street(address):\n",
    "    if pd.isnull(address):\n",
    "        return None\n",
    "\n",
    "    #Lowercase and remove house number\n",
    "    address = re.sub(r'^\\d+\\s*', '', address.strip().lower())\n",
    "\n",
    "    words = address.split()\n",
    "    result = []\n",
    "\n",
    "    for word in words:\n",
    "        #Remove punctuation\n",
    "        word = re.sub(r'[^\\w]', '', word)\n",
    "\n",
    "        #Normalize directionals first\n",
    "        if word in direction_map:\n",
    "            result.append(direction_map[word])\n",
    "        elif word in suffix_map:\n",
    "            result.append(suffix_map[word])\n",
    "        else:\n",
    "            result.append(word.capitalize())\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "trash_df['Street Name'] = trash_df['Incident Address'].astype(str).apply(\n",
    "    lambda x: re.sub(r'^\\d+\\s*', '', x.strip().lower()) if pd.notnull(x) else x\n",
    ")\n",
    "\n",
    "def safe_mode(series):\n",
    "    modes = series.mode()\n",
    "    return modes.iloc[0] if not modes.empty else None\n",
    "\n",
    "cols_to_infer = ['Zip Code', 'Trash Hauler', 'Trash Route', 'Council District']\n",
    "for col in cols_to_infer:\n",
    "    street_lookup = trash_df.groupby('Street Name')[col].agg(safe_mode)\n",
    "    trash_df[col] = trash_df[col].fillna(trash_df['Street Name'].map(street_lookup))\n",
    "\n",
    "\n",
    "#Creates new 'Street Name' column\n",
    "trash_df['Street Name'] = trash_df['Incident Address'].apply(normalize_street)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df4d2471-11f8-4d9f-aae1-c829f60fe7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I noticed that several other descriptions mention missed pickups in the Description column.\n",
    "#This is a list of keywords it finds in the event that the Request is not properly labeled as a Missed Pickup.\n",
    "#More keywords could be added if noticed in the raw data.\n",
    "\n",
    "import re\n",
    "\n",
    "keywords = [\n",
    "    'miss', 'missed', 'not picked up', 'not collected', 'no pickup',\n",
    "    'never came', 'did not collect', 'didn’t pick up', 'did not pick up'\n",
    "]\n",
    "\n",
    "pattern = '|'.join(re.escape(word) for word in keywords)\n",
    "\n",
    "is_missed_request = trash_df['Request '].str.contains(\"missed pickup\", case=False, na=False)\n",
    "is_missed_description = trash_df['description_clean'].str.contains(pattern, na=False)\n",
    "\n",
    "missed_pickups = trash_df[is_missed_request | is_missed_description]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cda4e75-2c80-4f81-a0d9-27398d35c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I noticed that some entries seemed to be duplicates based on the date opened and the Incident Address in the raw data.\n",
    "#This filters out any Requests that may have been submitted multiple times for the same Date/Incident Address\n",
    "missed_pickups = missed_pickups.copy()\n",
    "missed_pickups['Date Opened'] = pd.to_datetime(\n",
    "    missed_pickups['Date Opened'], format='%m/%d/%Y', errors='coerce'\n",
    ")\n",
    "\n",
    "unique_missed = missed_pickups.drop_duplicates(subset=['Incident Address', 'Date Opened'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05e1f661-5ed2-4234-b5b5-2e9c78aced45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I noticed some data was missing, i.e. Zip Code, Trash Hauler, Route, District etc.\n",
    "#This groups by the newly created 'Street Name' column and uses the infer data for the Street Name to fill in incomplete data if other instances of the same Street.\n",
    "cols_to_fill = ['Zip Code', 'Trash Hauler', 'Trash Route', 'Council District', 'State Plan X', 'State Plan Y']\n",
    "\n",
    "for col in cols_to_fill:\n",
    "    trash_df[col] = trash_df.groupby('Street Name')[col].transform(lambda x: x.ffill().bfill())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e780d93e-04ac-4aab-8fdf-42448478bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This exports the cleaned data to a CSV file. The output only includes unique missed pickups based on Request and/or keywords in Description columns.\n",
    "\n",
    "#Filter based on Request type or keywords in Description\n",
    "keywords = [\n",
    "    'miss', 'missed', 'not picked up', 'not collected', 'no pickup',\n",
    "    'never came', 'did not collect', 'didn’t pick up', 'did not pick up'\n",
    "]\n",
    "pattern = '|'.join(re.escape(word) for word in keywords)\n",
    "\n",
    "is_missed_request = trash_df['Request '].str.contains(\"missed pickup\", case=False, na=False)\n",
    "is_missed_description = trash_df['description_clean'].str.contains(pattern, na=False)\n",
    "\n",
    "missed_pickups = trash_df[is_missed_request | is_missed_description].copy()\n",
    "\n",
    "#Normalize data\n",
    "missed_pickups['Date Opened'] = pd.to_datetime(missed_pickups['Date Opened'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "#Remove same-day duplicates\n",
    "unique_missed = missed_pickups.drop_duplicates(subset=['Incident Address', 'Date Opened']).copy()\n",
    "\n",
    "#Normalize Street Name\n",
    "unique_missed['Street Name'] = unique_missed['Incident Address'].apply(normalize_street)\n",
    "\n",
    "#Fill missing data based on same Street Names\n",
    "cols_to_fill = ['Zip Code', 'Trash Hauler', 'Trash Route', 'Council District', 'State Plan X', 'State Plan Y']\n",
    "for col in cols_to_fill:\n",
    "    unique_missed[col] = unique_missed.groupby('Street Name')[col].transform(lambda x: x.ffill().bfill())\n",
    "    \n",
    "#Export to /data folder\n",
    "unique_missed.to_csv('../data/cleaned_missed_pickups.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a68dc8-929d-4347-9462-061dc5c97541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geospatial]",
   "language": "python",
   "name": "conda-env-geospatial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
